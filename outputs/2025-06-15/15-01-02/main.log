[2025-06-15 15:01:02,612][root][INFO] - Loaded configuration:
[2025-06-15 15:01:02,612][root][INFO] - {'seed': 42, 'checkpoint_dir': 'adapter', 'experiment_name': '${models}_eval', 'logger': {'wandb_mode': 'disabled', 'project': 'ensamble-semantic-uncertainty', 'entity': 'keram', 'log_model': False, 'logging_steps': 10, 'report_to': 'wandb'}, 'models': {'llama3': {'name': 'llama', 'version': 3.2, 'size': '1B', 'model_name': 'meta-llama/Llama-3.2-1B', 'model_it_name': 'meta-llama/Llama-3.2-1B-Instruct', 'model_spec': {'_target_': 'transformers.AutoModelForCausalLM.from_pretrained', 'pretrained_model_name_or_path': '${models.llama3.model_name}'}, 'tokenizer_spec': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${models.llama3.model_name}'}}, 'llama3_second_instance': {'name': 'llama', 'version': 3.2, 'size': '1B', 'model_name': 'meta-llama/Llama-3.2-1B', 'model_it_name': 'meta-llama/Llama-3.2-1B-Instruct', 'model_spec': {'_target_': 'transformers.AutoModelForCausalLM.from_pretrained', 'pretrained_model_name_or_path': '${models.llama3.model_name}'}, 'tokenizer_spec': {'_target_': 'transformers.AutoTokenizer.from_pretrained', 'pretrained_model_name_or_path': '${models.llama3.model_name}'}}}}
